# Screeps Clockwork Benchmarking System v2\n\nA comprehensive, generic benchmarking framework for comparing multiple implementations of the same functionality. Perfect for testing performance and quality of different algorithms, optimization techniques, or any functions that solve the same problem.\n\n## Features\n\n- **Generic Design**: Works with any function signature and return type\n- **Multiple Implementations**: Compare as many implementations as you want against a reference\n- **Custom Validation**: Ensure all implementations produce correct results\n- **Flexible Metrics**: Track custom metrics beyond just CPU time\n- **Tick-based Execution**: Spreads work across multiple game ticks to avoid CPU timeouts\n- **Quality Analysis**: Not just speed - measure accuracy, path cost, or any quality metric\n- **Rich Results**: Detailed comparisons with win rates, beat-reference rates, and more\n\n## Quick Start\n\n```typescript\nimport { registerBenchmark } from '../helpers/benchmark-v2';\n\n// Compare different sine implementations\nregisterBenchmark('Math Functions', (mark) => {\n  mark\n    .reference((x: number) => Math.sin(x))  // The \"ground truth\" implementation\n    .implement('cached-sin', cachedSin)     // Your optimized version\n    .implement('approx-sin', approxSin)     // Your approximate version\n    .testCase('small-numbers', () => generateRandomNumbers(1000, -Math.PI, Math.PI))\n    .validate((result, input, reference) => {\n      const error = Math.abs(result - reference);\n      return error < 0.001 ? true : `Error too large: ${error}`;\n    })\n    .minimize('absolute-error', (result, input, reference) => Math.abs(result - reference))\n    .iterations(10);\n});\n```\n\n## Core Concepts\n\n### 1. Reference Implementation\nThe \"ground truth\" that other implementations are compared against. Should be the most reliable, even if not the fastest.\n\n### 2. Implementations\nAlternative versions you want to test. These are compared both against each other and the reference.\n\n### 3. Test Cases\nDifferent scenarios with different input data. Each test case generates an array of inputs to test with.\n\n### 4. Validation\nEnsures implementations produce correct results. Failed validations are excluded from performance metrics.\n\n### 5. Metrics\nCustom measurements beyond CPU time. Three modes:\n- **minimize**: Lower values win (e.g., error, cost)\n- **maximize**: Higher values win (e.g., score, efficiency)  \n- **match**: Closer to reference wins (e.g., exact correctness)\n\n## API Reference\n\n### Basic Setup\n\n```typescript\nregisterBenchmark('Benchmark Name', (mark) => {\n  mark\n    .reference(referenceFn)           // Required: reference implementation\n    .implement('name', implFn)        // Add implementations to test\n    .testCase('case', generateInputs) // Add test cases\n    .validate(validationFn)           // Optional: validate correctness\n    .minimize('metric', gradeFn)      // Add custom metrics\n    .iterations(5);                   // Set averaging iterations\n});\n```\n\n### Method Details\n\n#### `.reference(fn: (input: T) => R)`\nSet the reference implementation. Required.\n\n```typescript\n.reference((x: number) => Math.sin(x))\n```\n\n#### `.implement(name: string, fn: (input: T) => R)`  \nAdd an implementation to benchmark. Can call multiple times.\n\n```typescript\n.implement('optimized-version', myOptimizedFn)\n.implement('approximate-version', myApproxFn)\n```\n\n#### `.testCase(name: string, generate: () => T[])`\nAdd a test case that generates input data. Can call multiple times.\n\n```typescript\n.testCase('small-inputs', () => Array(100).fill(0).map(() => Math.random()))\n.testCase('large-inputs', () => Array(50).fill(0).map(() => Math.random() * 1000))\n```\n\n#### `.validate(fn: (result: R, input: T, reference: R) => true | string)`\nValidate implementation results. Return `true` for valid, error string for invalid.\n\n```typescript\n.validate((result, input, reference) => {\n  if (Math.abs(result - reference) > 0.001) {\n    return `Too inaccurate: ${result} vs ${reference}`;\n  }\n  return true;\n})\n```\n\n#### `.minimize(name: string, grade: (result: R, input: T, reference: R) => number)`\nAdd a metric where lower values are better.\n\n```typescript\n.minimize('error', (result, input, reference) => Math.abs(result - reference))\n.minimize('path-cost', (result) => calculatePathCost(result))\n```\n\n#### `.maximize(name: string, grade: (result: R, input: T, reference: R) => number)`\nAdd a metric where higher values are better.\n\n```typescript\n.maximize('efficiency', (result, input) => calculateEfficiency(result))\n.maximize('score', (result) => result.score)\n```\n\n#### `.match(name: string, grade: (result: R, input: T, reference: R) => number)`  \nAdd a metric where closer to reference is better.\n\n```typescript\n.match('exact-length', (result, input, reference) => result.length)\n```\n\n#### `.iterations(count: number)`\nSet how many times to run each test for averaging. Default: 1.\n\n#### `.beforeEach(fn: (input: T) => void)` / `.afterEach(fn: (input: T) => void)`\nSetup and teardown for each test.\n\n```typescript\n.beforeEach((input) => clearCache())\n.afterEach((input) => logDebugInfo(input))\n```\n\n#### `.cpuLimit(fraction: number)`\nSet CPU limit as fraction of Game.cpu.limit. Default: 0.8 (80%).\n\n## Examples\n\n### 1. Simple Math Functions\n\n```typescript\nregisterBenchmark('Sine Implementations', (mark) => {\n  mark\n    .reference((x: number) => Math.sin(x))\n    .implement('lookup-table', lookupTableSin)\n    .implement('taylor-series', taylorSeriesSin)\n    .testCase('small-angles', () => \n      Array(1000).fill(0).map(() => Math.random() * Math.PI))\n    .testCase('full-range', () => \n      Array(1000).fill(0).map(() => Math.random() * 2 * Math.PI - Math.PI))\n    .validate((result, input, reference) => {\n      const error = Math.abs(result - reference);\n      return error < 0.0001 ? true : `Error: ${error.toFixed(6)}`;\n    })\n    .minimize('absolute-error', (result, input, reference) => \n      Math.abs(result - reference))\n    .minimize('relative-error', (result, input, reference) => \n      Math.abs((result - reference) / reference))\n    .iterations(10);\n});\n```\n\n### 2. Pathfinding Algorithms\n\n```typescript\ninterface PathInput {\n  from: RoomPosition;\n  to: RoomPosition[];\n}\n\nregisterBenchmark('Pathfinding', (mark) => {\n  mark\n    .reference((input: PathInput) => PathFinder.search(input.from, input.to[0]).path)\n    .implement('clockwork-astar', clockworkAstar)\n    .implement('clockwork-dijkstra', clockworkDijkstra)\n    .testCase('same-room', () => generateSameRoomPaths(100))\n    .testCase('cross-room', () => generateCrossRoomPaths(50))\n    .validate((result, input, reference) => {\n      if (!result || result.length === 0) {\n        return !reference || reference.length === 0 ? true : 'No path found';\n      }\n      return pathIsValid(result, input.from, input.to[0]) ? true : 'Invalid path';\n    })\n    .minimize('path-length', (result) => result ? result.length : Infinity)\n    .minimize('path-cost', (result) => result ? calculatePathCost(result) : Infinity)\n    .iterations(3);\n});\n```\n\n### 3. Caching vs Non-Caching\n\n```typescript\n// Test if caching actually helps\nconst cache = new Map();\n\nregisterBenchmark('Expensive Calculation', (mark) => {\n  mark\n    .reference((x: number) => expensiveCalculation(x))\n    .implement('with-cache', (x: number) => {\n      if (cache.has(x)) return cache.get(x);\n      const result = expensiveCalculation(x);\n      cache.set(x, result);\n      return result;\n    })\n    .testCase('random-inputs', () => \n      Array(1000).fill(0).map(() => Math.floor(Math.random() * 100)))\n    .testCase('repeated-inputs', () => {\n      const base = [1, 2, 3, 4, 5];\n      return Array(1000).fill(0).map(() => base[Math.floor(Math.random() * base.length)]);\n    })\n    .beforeEach(() => cache.clear())\n    .match('correctness', (result, input, reference) => result);\n});\n```\n\n## Understanding Results\n\nThe benchmark displays results in a table format:\n\n```\nBenchmark Results: Math Functions\n============================================\n\nCase: small-numbers (1000 tests, 10 iterations each)\nImplementation     │  Avg CPU │ CPU Wins % │ Beat Ref % │ Error │ Wins % │ Beat % │ Status\n──────────────────────────────────────────────────────────────────────────────────────\nreference          │    0.150 │      25.0% │   baseline │ 0.000 │  100.0% │  100.0% │ ✓\ncached-sin         │    0.120 │      45.0% │     80.0% │ 0.000 │   35.0% │  100.0% │ ✓\napprox-sin         │    0.080 │      30.0% │     95.0% │ 0.001 │   15.0% │   85.0% │ ✓\n```\n\n**Columns Explained:**\n- **Avg CPU**: Average CPU time per test\n- **CPU Wins %**: Percentage of tests where this impl had the best CPU time\n- **Beat Ref %**: Percentage of tests where this impl beat the reference (N/A for reference)\n- **Custom Metrics**: Your defined metrics with average value, win rate, and beat-reference rate\n- **Status**: ✓ for no failures, ✗ (count) for validation failures\n\n## Best Practices\n\n### 1. Choose Good Reference Implementation\n- Use the most reliable/correct implementation as reference\n- Don't worry if it's not the fastest - that's what you're testing\n- Reference should rarely fail validation\n\n### 2. Design Realistic Test Cases\n- Include edge cases and typical usage patterns\n- Use appropriate data sizes (not too small, not too large)\n- Consider different scenarios that might favor different approaches\n\n### 3. Write Robust Validation\n- Allow for reasonable tolerance in floating-point comparisons\n- Consider multiple \"correct\" answers (e.g., different but equally good paths)\n- Provide helpful error messages for debugging\n\n### 4. Choose Meaningful Metrics\n- CPU time is automatically tracked\n- Add metrics that matter for your use case (accuracy, path cost, memory usage)\n- Use appropriate metric modes (minimize error, maximize efficiency, match reference)\n\n### 5. Set Appropriate Iterations\n- More iterations = more accurate averages but slower execution\n- Start with 1-3 for development, increase for final comparisons\n- Consider the inherent variance in your implementations\n\n## Advanced Usage\n\n### Custom CPU Measurement\n\n```typescript\n// For testing outside Screeps environment\nconst runner = new BenchmarkRunner(config, {\n  getCpuUsed: () => performance.now(),\n  logger: console.log\n});\n```\n\n### Manual Benchmark Control\n\n```typescript\n// Instead of registerBenchmark, create benchmark manually\nconst bench = benchmark('Custom Benchmark')\n  .reference(refFn)\n  .implement('impl1', fn1)\n  .testCase('case1', generateData);\n\n// Run manually\nwhile (!bench.run()) {\n  // Tick-based execution\n}\n\nbench.displayResults();\nconst results = bench.getResults();\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **\"Reference function is required\"**: Make sure to call `.reference(fn)` before `.run()`\n\n2. **\"At least one implementation is required\"**: Add implementations with `.implement(name, fn)`\n\n3. **\"At least one test case is required\"**: Add test cases with `.testCase(name, generator)`\n\n4. **All implementations failing validation**: Check your validation function logic\n\n5. **Inconsistent CPU measurements**: Ensure consistent game state between tests\n\n### Performance Tips\n\n1. **CPU Timeouts**: Reduce `.cpuLimit()` or `.iterations()` if hitting limits\n\n2. **Memory Usage**: Clear caches in `.beforeEach()` if testing caching behavior\n\n3. **Deterministic Results**: Use fixed seeds for random data when possible\n\n## Migration from v1\n\nThe old benchmark system used a more complex API. Here's how to migrate:\n\n**Old System:**\n```typescript\nbench('pathfinder', pathfinder, (mark) => {\n  mark.test('case', () => generateData());\n  mark.implement('impl1', fn1);\n  mark.validate((result, args, ref) => result.length > 0);\n  mark.minimize('length', (result) => result.length);\n});\n```\n\n**New System:**\n```typescript\nregisterBenchmark('pathfinder', (mark) => {\n  mark\n    .reference(pathfinder)\n    .testCase('case', () => generateData())\n    .implement('impl1', fn1)\n    .validate((result, input, ref) => result.length > 0 ? true : 'No path')\n    .minimize('length', (result) => result.length);\n});\n```\n\n**Key Changes:**\n- `bench()` → `registerBenchmark()`\n- Reference function moved to first parameter of `.reference()`\n- Validation returns `true | string` instead of `true | string`\n- More fluent/chainable API\n- Better TypeScript support\n"